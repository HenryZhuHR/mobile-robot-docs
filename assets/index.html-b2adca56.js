import{_ as c}from"./lane-define-68142137.js";import{_ as p,r as l,o as d,c as u,a as e,d as n,b as a,f as r,w as i,e as t}from"./app-0530136c.js";const h={},b=t('<h1 id="车道线检测深度学习方法" tabindex="-1"><a class="header-anchor" href="#车道线检测深度学习方法" aria-hidden="true">#</a> 车道线检测深度学习方法</h1><ul><li><a href="#%E8%BD%A6%E9%81%93%E7%BA%BF%E6%A3%80%E6%B5%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95">车道线检测深度学习方法</a><ul><li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li><li><a href="#%E9%A1%B9%E7%9B%AE">项目</a></li><li><a href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83">配置环境</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87">数据集准备</a></li><li><a href="#%E8%AE%AD%E7%BB%83">训练</a></li><li><a href="#%E6%B5%8B%E8%AF%95">测试</a></li><li><a href="#%E9%83%A8%E7%BD%B2">部署</a><ul><li><a href="#1-onnx-%E9%83%A8%E7%BD%B2">1. ONNX 部署</a></li><li><a href="#2-tensorrt-%E9%83%A8%E7%BD%B2">2. TensorRT 部署</a><ul><li><a href="#%E5%AE%89%E8%A3%85-tensorrt-%E7%8E%AF%E5%A2%83">安装 TensorRT 环境</a></li><li><a href="#1-tensorrt-%E6%8E%A8%E7%90%86-onnx-%E6%A8%A1%E5%9E%8B">1. TensorRT 推理 ONNX 模型</a></li><li><a href="#2-tensorrt-%E6%8E%A8%E7%90%86-engine-%E6%A8%A1%E5%9E%8B">2. TensorRT 推理 Engine 模型</a></li></ul></li></ul></li><li><a href="#%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">问题与解决方案</a><ul><li><a href="#tensor-rt-engine-%E8%BD%AC%E6%8D%A2%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98">Tensor RT Engine 转换过程中的问题</a></li></ul></li><li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li></ul></li></ul><h2 id="任务" tabindex="-1"><a class="header-anchor" href="#任务" aria-hidden="true">#</a> 任务</h2><p><img src="'+c+'" alt="culane"></p><p>车道线检测 (Lane Detetction) ：需要将视频中出现的车道线检测出来。任务要求如下:</p><ol><li>查询车道线检测的相关综述，包括<code>传统视觉</code>和<code>深度学习</code>的方案</li><li>通常需要检测四条车道线：左车道、自车道、右车道。</li><li>需要判断双黄线、白色虚线、白色实线</li><li>如果车道线终止，需要判断出结束位置</li><li>车道线需要拟合成曲线，检测的车道线要求稳定，不能发生连续帧之间突变的情况</li><li>测试视频为 <a href="../../examples/lane-detection-demo.mp4"><code>examples/lane-detection-demo.mp4</code></a>，测试结果仍然保存为视频</li></ol><h2 id="项目" tabindex="-1"><a class="header-anchor" href="#项目" aria-hidden="true">#</a> 项目</h2>',7),m={href:"https://github.com/cfzd/Ultra-Fast-Lane-Detection",target:"_blank",rel:"noopener noreferrer"},v=t(`<h2 id="配置环境" tabindex="-1"><a class="header-anchor" href="#配置环境" aria-hidden="true">#</a> 配置环境</h2><blockquote><p>下所有的命令(包括对数据集的处理)都在当前路径操作，相对于 mobile-robot 项目，为 <code>modules/lane-detection</code>，因此请 <code>pwd</code>检查是否处于该路径，否则 <code>cd ./modules/lane-detection</code></p></blockquote><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>conda create <span class="token parameter variable">-n</span> lanedet <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span> <span class="token parameter variable">-y</span>
conda activate lanedet
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,3),_={href:"https://pytorch.org",target:"_blank",rel:"noopener noreferrer"},g=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> torch torchvision --index-url https://download.pytorch.org/whl/cu118
<span class="token comment"># or</span>
conda <span class="token function">install</span> pytorch torchvision pytorch-cuda<span class="token operator">=</span><span class="token number">11.8</span> <span class="token parameter variable">-c</span> pytorch <span class="token parameter variable">-c</span> nvidia <span class="token parameter variable">-y</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>测试时：<code>torch-2.0.1+cu118</code> <code>torchvision-0.15.2+cu118</code></p></blockquote><p>安装其他依赖</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip3 <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="数据集准备" tabindex="-1"><a class="header-anchor" href="#数据集准备" aria-hidden="true">#</a> 数据集准备</h2><p>车道线检测常用的公开数据集:</p>`,6),k={href:"https://xingangpan.github.io/projects/CULane.html",target:"_blank",rel:"noopener noreferrer"},f={href:"https://www.kaggle.com/datasets/manideep1108/tusimple?resource=download",target:"_blank",rel:"noopener noreferrer"},T={href:"https://github.com/SoulmateB/CurveLanes",target:"_blank",rel:"noopener noreferrer"},E={href:"https://xingangpan.github.io/projects/CULane.html",target:"_blank",rel:"noopener noreferrer"},x=t(`<p>无论是使用 <strong>CULane 数据集</strong>还是<strong>自制数据集</strong>，都需要设置环境变量 <code>$CULANEROOT</code></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CULANEROOT</span><span class="token operator">=</span>/path/to/culane
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="训练" tabindex="-1"><a class="header-anchor" href="#训练" aria-hidden="true">#</a> 训练</h2><p>首先，修改配置文件 <code>configs/culane.py</code> 中重要的参数（复制一份配置文件到 <code>temp/culane.py</code> 修改，而不是修改原始的配置文件），包括：</p>`,4),A=t("<li><code>data_root</code> 是 CULane 数据集路径</li><li><code>log_path</code> 是输出的模型路径，这里默认为 <code>tmps</code></li><li>训练中的超参数，比如 <code>epoch</code>,<code>batch_size</code>,<code>steps</code> 等</li>",3),N=e("code",null,"resume",-1),R={href:"https://drive.google.com/file/d/1zXBRTw50WOzvUp6XKsi8Zrk3MUC3uFuq/view?usp=sharing",target:"_blank",rel:"noopener noreferrer"},y={href:"https://pan.baidu.com/s/19Ig0TrV8MfmFTyCvbSa4ag?pwd=w9tw",target:"_blank",rel:"noopener noreferrer"},D=t(`<p>确认配置文件无误后启动训练脚本，需要修改脚本 <code>temp/culane.py</code> 内</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 train.py temp/culane.py <span class="token comment"># 单 GPU 训练</span>
<span class="token function">bash</span> scripts/train-dist.sh      <span class="token comment"># 多 GPU 训练</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="测试" tabindex="-1"><a class="header-anchor" href="#测试" aria-hidden="true">#</a> 测试</h2><p>训练完成后，需要对模型进行测试，这里是对 <code>$CULANEROOT</code> 进行测试，如果只希望对单张图像进行测试，在下文提到。需要修改 <code>temp/culane.py</code> 中 <code>test_model</code>(待测试的模型权重文件) 和 <code>test_work_dir</code>(测试结果输出的目录，默认<code>tmps</code>)，然后运行</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 test.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>单张图像测试，需要配置的内容与上述结果相同，但是需要修改文件中 <code>test_img = &quot;datasets/CULane/images/04980.jpg&quot;</code> 为指定的图像</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 infer-torch.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="部署" tabindex="-1"><a class="header-anchor" href="#部署" aria-hidden="true">#</a> 部署</h2><p>部署分为两种方式：</p><ol><li><strong>ONNX 部署</strong>：将 pytorch 模型转化为 ONNX 模型，然后使用 ONNXRuntime 进行推理</li><li><strong>TensorRT 部署</strong>：：将 pytorch 模型转化为 ONNX/Engine 模型，然后使用 TensorRT 进行推理</li></ol><p>这里给出一种推理速度参考</p><table><thead><tr><th style="text-align:center;">推理方式</th><th style="text-align:center;">平均推理时间</th></tr></thead><tbody><tr><td style="text-align:center;">Pytorch</td><td style="text-align:center;">30.9649 ms</td></tr><tr><td style="text-align:center;">ONNXRuntime</td><td style="text-align:center;">19.9175 ms</td></tr><tr><td style="text-align:center;">TensorRT Engine</td><td style="text-align:center;">6.5350 ms</td></tr></tbody></table>`,12),B={href:"https://developer.nvidia.com/embedded/jetson-nano-developer-kit",target:"_blank",rel:"noopener noreferrer"},O={href:"https://developer.nvidia.com/embedded/jetpack-sdk-461",target:"_blank",rel:"noopener noreferrer"},C=e("li",null,[e("strong",null,"OS"),n(": Ubuntu 18.04, Linux kernel 4.9")],-1),U={href:"https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-821/quick-start-guide/index.html",target:"_blank",rel:"noopener noreferrer"},L=e("strong",null,"TensorRT 8.2.1",-1),I=e("li",null,[e("strong",null,"cuDNN 8.2.1")],-1),P={href:"https://docs.nvidia.com/cuda/archive/10.2/cuda-toolkit-release-notes/index.html#title-new-features",target:"_blank",rel:"noopener noreferrer"},H=e("strong",null,"CUDA 10.2",-1),w=t(`<h3 id="_1-onnx-部署" tabindex="-1"><a class="header-anchor" href="#_1-onnx-部署" aria-hidden="true">#</a> 1. ONNX 部署</h3><p>修改配置文件 <code>temp/culane.py</code> 中 <code>fintune</code> 为 pytorch 模型的权重，运行脚本后会在相同目录下生成同名的 <code>.onnx</code> 文件</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 export.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>得到 <code>.onnx</code> 文件后，修改推理脚本 <code>infer-onnx.py</code> 中的 onnx 模型权重 <code>onnx_file</code> 和待测试视频 <code>video</code>，然后运行如下</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 infer-onnx.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_2-tensorrt-部署" tabindex="-1"><a class="header-anchor" href="#_2-tensorrt-部署" aria-hidden="true">#</a> 2. TensorRT 部署</h3>`,6),$={href:"https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_8",target:"_blank",rel:"noopener noreferrer"},M=e("ol",null,[e("li",null,"使用 TensorRT 推理 ONNX 模型"),e("li",null,"将 ONNX 转化为 Engine 模型并使用 TensorRT 推理")],-1),S={href:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html",target:"_blank",rel:"noopener noreferrer"},X={href:"https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html",target:"_blank",rel:"noopener noreferrer"},z={href:"https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/index.html",target:"_blank",rel:"noopener noreferrer"},q=e("h4",{id:"安装-tensorrt-环境",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#安装-tensorrt-环境","aria-hidden":"true"},"#"),n(" 安装 TensorRT 环境")],-1),J=e("p",null,"必须 Ubuntu + Nvidia GPU 环境",-1),V={href:"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar",target:"_blank",rel:"noopener noreferrer"},F={href:"https://developer.nvidia.cn/login",target:"_blank",rel:"noopener noreferrer"},Y=e("p",null,[e("strong",null,"安装 CUDA")],-1),j={href:"https://developer.nvidia.com/cuda-toolkit-archive",target:"_blank",rel:"noopener noreferrer"},G=e("code",null,"deb(local)",-1),W=e("strong",null,"注意",-1),K={href:"https://developer.nvidia.com/cuda-10.2-download-archive",target:"_blank",rel:"noopener noreferrer"},Z={href:"https://developer.nvidia.com/cuda-11-4-0-download-archive",target:"_blank",rel:"noopener noreferrer"},Q=e("p",null,[e("strong",null,"安装 cuDNN")],-1),ee={href:"https://developer.nvidia.com/rdp/cudnn-archive",target:"_blank",rel:"noopener noreferrer"},ne=e("strong",null,"CUDA 版本",-1),ae=e("strong",null,"系统架构",-1),se=e("strong",null,"Tar",-1),te={href:"https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.1.32/10.2_06072021/cudnn-10.2-linux-x64-v8.2.1.32.tgz",target:"_blank",rel:"noopener noreferrer"},oe=e("code",null,"cuDNN v8.2.1, for CUDA 10.2, for Linux (x86)",-1),le={href:"https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.1.32/11.3_06072021/cudnn-11.3-linux-x64-v8.2.1.32.tgz",target:"_blank",rel:"noopener noreferrer"},re=e("code",null,"cuDNN v8.2.1, for CUDA 11.x, for Linux (x86)",-1),ie={href:"https://developer.nvidia.com/downloads/compute/cudnn/secure/8.9.0/local_installers/11.8/cudnn-linux-x86_64-8.9.0.131_cuda11-archive.tar.xz/",target:"_blank",rel:"noopener noreferrer"},ce=e("code",null,"cuDNN8.9.0 + CUDA 11.8 + Linux x86_64 (Tar)",-1),pe=t('<p>下载后得到压缩包 <code>cudnn-${version}.tar.xz</code> ，将压缩包解压后得到同名的目录，但是8.2.1 解压后得到的是 cuda 目录，建议改名</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">tar</span> <span class="token parameter variable">-xf</span> cudnn-<span class="token variable">${version}</span>.tar.xz\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>',2),de=e("p",null,[e("strong",null,"安装 TensorRT")],-1),ue={href:"https://developer.nvidia.com/tensorrt-getting-started",target:"_blank",rel:"noopener noreferrer"},he=e("em",null,"Download Now",-1),be={href:"https://developer.nvidia.com/compute/machine-learning/tensorrt/secure/8.2.1/tars/tensorrt-8.2.1.8.linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz",target:"_blank",rel:"noopener noreferrer"},me=e("code",null,"TensorRT 8.2 GA for Linux x86_64 and CUDA 11.0-5 TAR Package",-1),ve=t('<p>下载后得到压缩包 <code>TensorRT-${version}.tar.gz</code> ，将压缩包解压后得到同名的目录</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">tar</span> <span class="token parameter variable">-xf</span> TensorRT-<span class="token variable">${version}</span>.tar.gz\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div>',2),_e=t(`<p>在系统环境变量中添加 CUDA, cuDNN,TensorRT 相关的路径(修改为真实路径<code>$xxx_HOME</code>)。添加完成后，<code>source ~/.bashrc</code> 使环境变量生效</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token comment"># ------ CUDA ------</span>
<span class="token assign-left variable">CUDA_VERSION</span><span class="token operator">=</span><span class="token number">11.8</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda-<span class="token variable">\${CUDA_VERSION}</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/lib64
<span class="token comment"># ------ cuDNN 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDNN_HOME</span><span class="token operator">=</span>path/to/cudnn-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span> 
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDNN_HOME</span>/lib64 <span class="token comment"># 新版本为 lib 目录，建议自己检查一下</span>
<span class="token comment"># ------ TensorRT 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TENSORRT_HOME</span><span class="token operator">=</span>path/to/TensorRT-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/lib
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/bin
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>安装 python相关环境</li></ol>`,3),ge={href:"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip",target:"_blank",rel:"noopener noreferrer"},ke={href:"https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip",target:"_blank",rel:"noopener noreferrer"},fe=e("code",null,"tensorrt",-1),Te=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 根据 python 版本选择对应的 .whl 文件</span>
python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> <span class="token variable">$TENSORRT_HOME</span>/python/tensorrt-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span>.whl
python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pycuda<span class="token operator">&gt;=</span><span class="token number">2020.1</span> <span class="token comment"># 已写入 requirements.txt</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_1-tensorrt-推理-onnx-模型" tabindex="-1"><a class="header-anchor" href="#_1-tensorrt-推理-onnx-模型" aria-hidden="true">#</a> 1. TensorRT 推理 ONNX 模型</h4>`,2),Ee={href:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_topics",target:"_blank",rel:"noopener noreferrer"},xe=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h4 id="_2-tensorrt-推理-engine-模型" tabindex="-1"><a class="header-anchor" href="#_2-tensorrt-推理-engine-模型" aria-hidden="true">#</a> 2. TensorRT 推理 Engine 模型</h4>`,2),Ae={href:"https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-821/quick-start-guide/index.html#export-from-pytorch",target:"_blank",rel:"noopener noreferrer"},Ne={href:"https://github.com/NVIDIA/TensorRT/tree/release/8.2",target:"_blank",rel:"noopener noreferrer"},Re={href:"https://github.com/NVIDIA/TensorRT/blob/release/8.2/samples/python/efficientnet/requirements.txt",target:"_blank",rel:"noopener noreferrer"},ye=e("code",null,"requirements.txt",-1),De=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token assign-left variable">onnx</span><span class="token operator">==</span><span class="token number">1.9</span>.0 <span class="token comment"># 已写入 requirements.txt</span>
python3 export.py temp/culane.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>在转换之前，确保有 <code>**-INT32.onnx</code> 模型，因为 TensorRT 支持 INT32 而不支持 INT64。如果没有，可以使用 <code>onnxsim</code> 工具进行转换</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> onnxsim weights/culane_18.onnx weights/culane_18-sim.onnx
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>上述操作都可以在电脑上完成，但是如果需要在 Jetson Nano 上推理，则需要将 onnx 转换为 Engine 模型需要在 Jetson Nano 上完成，否则会出现<a href="#issuse-tensorrt-engine_incompatible_device">&quot;<em>不匹配设备的报错</em>&quot;</a>，但是后面的步骤可以在电脑上测试没有问题再在 Jetson Nano 上部署。</p><p>Jetpack 自带的库在 <code>/usr/src</code>，因此在 Jetson Nano 的系统环境变量中添加如下内容，然后<code>source ~/.bashrc</code> 使环境变量生效</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># ~/.bashrc</span>
<span class="token comment"># ------ CUDA 10.2 ------</span>
<span class="token assign-left variable">CUDA_VERSION</span><span class="token operator">=</span><span class="token number">10.2</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_HOME</span><span class="token operator">=</span>/usr/local/cuda-<span class="token variable">\${CUDA_VERSION}</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$CUDA_HOME</span>/lib64
<span class="token comment"># ------ TensorRT 8.2.1 ------</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">TENSORRT_HOME</span><span class="token operator">=</span>/usr/src/tensorrt
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/lib
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$TENSORRT_HOME</span>/bin
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,6),Be=e("code",null,"**-INT32.onnx",-1),Oe=e("code",null,"**-INT32.engine",-1),Ce=e("a",{href:"#TensorRT-Engine-%E8%BD%AC%E6%8D%A2%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98"},'"Tensor RT Engine 转换过程中的问题"',-1),Ue=t(`<div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 export.py configs/culane.py
trtexec <span class="token parameter variable">--verbose</span> <span class="token parameter variable">--fp16</span> <span class="token punctuation">\\</span>
  <span class="token parameter variable">--onnx</span><span class="token operator">=</span>weights/culane_18-INT32.onnx <span class="token punctuation">\\</span>
  <span class="token parameter variable">--saveEngine</span><span class="token operator">=</span>weights/culane_18-INT32.engine
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><code>--workspace=N</code>: Set workspace size in megabytes (default = 16)</li><li><code>--fp16</code>: Enable fp16 precision, in addition to fp32 (default = disabled)</li><li><code>--int8</code>: Enable int8 precision, in addition to fp32 (default = disabled)</li><li><code>--verbose</code>: Use verbose logging (default = false)</li><li><code>--exportTimes=&lt;file&gt;</code> :Write the timing results in a json file (default = disabled)</li><li><code>--exportOutput=&lt;file&gt;</code>: Write the output tensors to a json file (default = disabled)</li><li><code>--exportProfile=&lt;file&gt;</code>: Write the profile information per layer in a json file (default = disabled)</li></ul>`,2),Le={href:"https://github.com/NVIDIA-AI-IOT/torch2trt",target:"_blank",rel:"noopener noreferrer"},Ie=e("code",null,"torch2trt",-1),Pe=e("code",null,"PyTroch -> Engine",-1),He=e("a",{href:"#issuse-tensorrt-engine_incompatible_device"},"不匹配设备的报错",-1),we=e("code",null,"PyTroch -> ONNX",-1),$e=e("code",null,"ONNX",-1),Me=e("code",null,"trtexec",-1),Se=e("code",null,"ONNX -> Engine",-1),Xe=t(`<p>运行前需要安装 <code>pycuda</code></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> pycuda<span class="token operator">&gt;=</span><span class="token number">2020.1</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>然后修改 <code>deploy/infer-trtEngine.py</code> 中的 <code>TRT_MODEL_PATH</code> 为 <code>**-INT32.engine</code> 的路径，运行</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> deploy
python3 infer-trtEngine.py
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="问题与解决方案" tabindex="-1"><a class="header-anchor" href="#问题与解决方案" aria-hidden="true">#</a> 问题与解决方案</h2><h3 id="tensor-rt-engine-转换过程中的问题" tabindex="-1"><a class="header-anchor" href="#tensor-rt-engine-转换过程中的问题" aria-hidden="true">#</a> Tensor RT Engine 转换过程中的问题</h3>`,6),ze=t(`<p>报错如下</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token punctuation">[</span>07/13/2023-11:01:12<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> Error<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>: <span class="token punctuation">[</span>caskUtils.cpp::trtSmToCask::147<span class="token punctuation">]</span> Error Code <span class="token number">1</span>: Internal Error <span class="token punctuation">(</span>Unsupported SM: 0x809<span class="token punctuation">)</span>
<span class="token punctuation">[</span>07/13/2023-11:01:12<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> Error<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>: <span class="token punctuation">[</span>builder.cpp::buildSerializedNetwork::609<span class="token punctuation">]</span> Error Code <span class="token number">2</span>: Internal Error <span class="token punctuation">(</span>Assertion enginePtr <span class="token operator">!=</span> nullptr failed. <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,2),qe={href:"https://github.com/NVIDIA/TensorRT/issues/2727#issuecomment-1492809565",target:"_blank",rel:"noopener noreferrer"},Je=e("code",null,"Unsupported SM",-1),Ve=t(`<li><p>报错如下<span id="issuse-tensorrt-engine_incompatible_device"></span></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token punctuation">[</span>07/14/2023-11:41:43<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> <span class="token number">6</span>: The engine plan <span class="token function">file</span> is generated on an incompatible device, expecting compute <span class="token number">5.3</span> got compute <span class="token number">6.1</span>, please rebuild.
<span class="token punctuation">[</span>07/14/2023-11:41:43<span class="token punctuation">]</span> <span class="token punctuation">[</span>TRT<span class="token punctuation">]</span> <span class="token punctuation">[</span>E<span class="token punctuation">]</span> <span class="token number">4</span>: <span class="token punctuation">[</span>runtime.cpp::deserializeCudaEngine::50<span class="token punctuation">]</span> Error Code <span class="token number">4</span>: Internal Error <span class="token punctuation">(</span>Engine deserialization failed.<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>这是由于 Engine 模型不是在 Jetson nano 上生成的，在 Jetson nano 上重新生成 Engine 模型即可</p></li>`,1),Fe=e("h2",{id:"参考资料",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#参考资料","aria-hidden":"true"},"#"),n(" 参考资料")],-1),Ye={href:"https://github.com/Turoad/lanedet",target:"_blank",rel:"noopener noreferrer"},je={href:"https://zhuanlan.zhihu.com/p/353339637",target:"_blank",rel:"noopener noreferrer"},Ge={href:"https://github.com/qinnzou/Robust-Lane-Detection.git",target:"_blank",rel:"noopener noreferrer"};function We(Ke,Ze){const s=l("ExternalLinkIcon"),o=l("RouterLink");return d(),u("div",null,[b,e("p",null,[n("项目是基于"),e("a",m,[n("Ultra Fast Lane Detection"),a(s)])]),v,e("p",null,[n("安装"),e("a",_,[n("Pytorch"),a(s)]),n("依赖，需要 CUDA 11.8")]),g,e("ul",null,[e("li",null,[e("a",k,[n("CULane"),a(s)])]),e("li",null,[e("a",f,[n("TuSimple"),a(s)])]),e("li",null,[e("a",T,[n("CurveLanes"),a(s)]),n(":华为数据集 这里我们选择 "),e("a",E,[n("CULane"),a(s)]),n(" 作为参考进行实验、制作自己的数据集")])]),r(` 作者提供了[分割工具](https://github.com/XingangPan/seg_label_generate.git)
项目实现 [PytorchAutoDrive](https://github.com/voldemortX/pytorch-auto-drive) `),e("p",null,[n("CULane 数据集处理和制作参考"),a(o,{to:"/modules/lane-detection/docs/dataset-culane.html"},{default:i(()=>[n("CULane文档")]),_:1})]),x,e("ul",null,[A,e("li",null,[N,n(" 是预训练权重，如果需要加载预训练权重，可以下载官方的预训练权重: "),e("a",R,[n("GoogleDrive"),a(s)]),n("/"),e("a",y,[n("BaiduDrive"),a(s)]),n("，官方的预训练权重基于 resnet18，如果希望替换成其他 backbone，需要从头训练")])]),D,e("p",null,[n("TensorRT 部署在 Jetson 上需要考虑系统环境，例如 "),e("a",B,[n("Jetson Nano"),a(s)]),n(" 系统中包含"),e("a",O,[n("JetPack 4.6.1"),a(s)]),n(":")]),e("ul",null,[C,e("li",null,[e("a",U,[L,a(s)])]),I,e("li",null,[e("a",P,[H,a(s)])])]),w,e("p",null,[n("参考 "),e("a",$,[n("TensorRT 官方文档"),a(s)]),n("，部署有两种方式：")]),M,r(" https://zhuanlan.zhihu.com/p/527238167 "),e("p",null,[n("TensorRT官方文档 "),e("a",S,[n('"NVIDIA Official Documentation"'),a(s)])]),e("ul",null,[e("li",null,[e("a",X,[n("TensorRT Python API."),a(s)])]),e("li",null,[e("a",z,[n("TensorRT C++ API."),a(s)])])]),q,J,e("ol",null,[e("li",null,[n("安装 CUDA、cuDNN、TensorRT，参考"),e("a",V,[n("官方文档"),a(s)]),n("，需要注册 "),e("a",F,[n("Nvidia 账号"),a(s)]),n("，并登陆")])]),e("ul",null,[e("li",null,[Y,e("p",null,[n("进入"),e("a",j,[n("CUDA 下载页面"),a(s)]),n("并选择需要的版本，根据电脑的配置依次选择各项，最后选择 "),G,n("安装方式，将出现的命令依次复制到终端中执行。")]),e("p",null,[W,n(": JetPack 4.6.1 "),e("a",K,[n("CUDA 10.2"),a(s)]),n("，但是 10.2 不支持 Ubuntu20，所以下载 "),e("a",Z,[n("CUDA 11.4"),a(s)])])]),e("li",null,[Q,e("p",null,[n("进入"),e("a",ee,[n("cuDNN 下载页面"),a(s)]),n("，根据 "),ne,n("和"),ae,n("选择对应的版本的 "),se,n("文件，下载以下内文件：")]),e("ul",null,[e("li",null,[e("a",te,[oe,a(s)]),n(": 安装在 Jetson Nano 上用于转换 Engine 模型")]),e("li",null,[e("a",le,[re,a(s)]),n(": 安装在电脑上用于转换 Engine 模型和测试，这一步是为了在电脑上编写推理代码和测试，如果推理代码已经写好，可以不用安装")]),e("li",null,[e("a",ie,[ce,a(s)]),n(": 最新版本")])]),pe]),e("li",null,[de,e("p",null,[n("进入"),e("a",ue,[n("下载页面"),a(s)]),n("，点击 "),he,n(" 的入口，选择版本的 Tar 文件下载：")]),e("ul",null,[e("li",null,[e("a",be,[me,a(s)])])]),ve])]),_e,e("p",null,[n("参考官方文档 "),e("a",ge,[n('"Python Package Index Installation"'),a(s)]),n(" 安装 python 的"),e("a",ke,[fe,a(s)])]),Te,e("p",null,[n("参考官方"),e("a",Ee,[n("Python API"),a(s)])]),xe,e("p",null,[n("参考官方文档"),e("a",Ae,[n('"Exporting To ONNX From PyTorch"/"Converting ONNX To A TensorRT Engine"'),a(s)]),n("这种方式的部署流程是：首先将 ONNX 模型转化为 TensorRT Engine 模型，再使用 TensorRT API 推理 Engine 模型")]),e("p",null,[n("转换过程中需要考虑到 TensorRT 支持的 ONNX 版本，具体可以参考 "),e("a",Ne,[n("TensorRT(8.2.1) 源码"),a(s)]),n("中的 "),e("a",Re,[ye,a(s)]),n(" 文件，所以需要重新安装 ONNX 再重新转换")]),De,e("p",null,[n("使用官方提供的转换工具进行转换，如果 TensorRT 环境配置正确，将 "),Be,n(" 转化为 "),Oe,n("。转换过程中可能出现的bug以及解决方案记录在"),Ce,n("中，这里提供一个"),a(o,{to:"/modules/lane-detection/docs/onnx2engine.html"},{default:i(()=>[n("输出细节参考")]),_:1})]),Ue,e("blockquote",null,[e("p",null,[n("实际上，NV 官方提供了 "),e("a",Le,[Ie,a(s)]),n(" 转换工具可以直接完成 "),Pe,n("，但是在 Jetson Nano 上部署的时候，需要在 Jetson Nano 上完成模型转换，否则在实际使用时会出现"),He,n("，那么将完整的 torch 项目直接安装在 Jetson Nano 上可能会遇到很多问题，因此，这里先在电脑上完成 "),we,n(" 转换，然后将转换好的 "),$e,n(" 复制到 Jetson Nano 上使用 "),Me,n(" 完成 "),Se,n(" 转换，可以避免在 Jetson Nano 上安装 torch 环境和项目的一些其他环境。")])]),Xe,e("ul",null,[e("li",null,[ze,e("p",null,[n("参考 "),e("a",qe,[n("TensorRT #2727"),a(s)]),n("，"),Je,n(" 表示该版本的 TensorRT 不支持当前的 GPU 的 SM（SM是流媒体多处理器(Streaming Multiprocessor)，RTX40系列具有与以前的GPU系列不同的SM架构），需要升级 TensorRT 版本，或者选择比如RTX3080的GPU，TensorRT 8.5.1.7以上版本支持RTX40系SM")])]),Ve]),Fe,e("ul",null,[e("li",null,[e("p",null,[n("车道线检测项目"),e("a",Ye,[n("lanedet"),a(s)])])]),e("li",null,[e("p",null,[n("参考 百度 Apollo 项目的"),e("a",je,[n("车道线检测方法"),a(s)]),n("，使用深度学习方法进行车道线检测。")])]),e("li",null,[e("p",null,[n("CNN + LSTM "),e("a",Ge,[n("Robust-Lane-Detection"),a(s)])])])])])}const nn=p(h,[["render",We],["__file","index.html.vue"]]);export{nn as default};
